{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a89d39-c08a-4e55-8393-bc57db454bf3",
   "metadata": {},
   "source": [
    "<b>INDIVIDUAL ASSIGNMENT    \n",
    "Name: Heng Yi Sheng  \n",
    "TP Number: TP048930  \n",
    "Intake: UC3F2108CS(DA)</b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224969b8-bd72-44cc-909c-2bee10497bb6",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7568502e-8000-4e46-898f-a0f86f5f76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6297cb0-1b62-4489-9895-728fc7a5df44",
   "metadata": {},
   "source": [
    "### Read txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ee48f1-158b-46d1-bd5c-76829994d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is important for scientific, economic, social, and cultural reasons. NLP is experiencing rapid growth as its theories and methods are deployed in a variety of new language technologies. For this reason it is important for a wide range of people to have a working knowledge of NLP. Within industry, this includes people in human-computer interaction, business information analysis, and web software development. Within academia, it includes people in areas from humanities computing and corpus linguistics through to computer science and artificial intelligence. (To many people in academia, NLP is known by the name of \"Computational Linguistics.\")\n"
     ]
    }
   ],
   "source": [
    "# Read txt file\n",
    "file = open('D:\\Essien\\Degree Studies\\Degree Level 3\\Semester 1\\TXSA\\Individual Assignment\\Individual Assignment Data\\Data_1.txt')\n",
    "text = file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba46b5-3d8c-462d-95c0-39f9207b7a77",
   "metadata": {},
   "source": [
    "## Q1. FORM TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c151f6a-eb72-4aef-b19a-6a5c66fa7a9c",
   "metadata": {},
   "source": [
    "### 1. Sentence segmentation/sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7a3450-a210-4d81-9f87-da2aa2eaf1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -  NLP is important for scientific, economic, social, and cultural reasons.\n",
      "2 -  NLP is experiencing rapid growth as its theories and methods are deployed in a variety of new language technologies.\n",
      "3 -  For this reason it is important for a wide range of people to have a working knowledge of NLP.\n",
      "4 -  Within industry, this includes people in human-computer interaction, business information analysis, and web software development.\n",
      "5 -  Within academia, it includes people in areas from humanities computing and corpus linguistics through to computer science and artificial intelligence.\n",
      "6 -  (To many people in academia, NLP is known by the name of \"Computational Linguistics.\")\n",
      "\n",
      "Total sentences in the paragraph:  6\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.tokenize.sent_tokenize(text) # Perform sentence tokenization\n",
    "\n",
    "for i in range(len(sent)):\n",
    "    print(i + 1, \"- \", sent[i])\n",
    "    \n",
    "print('\\nTotal sentences in the paragraph: ', len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501574cd-b3a1-47de-9748-f63afc99c80f",
   "metadata": {},
   "source": [
    "### 2. Word Tokenization (Split Function, Regular Expression & NLTK package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c049af-26d7-4330-810d-549e4cd16706",
   "metadata": {},
   "source": [
    "#### a. Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205d0321-7631-4af5-ab5c-b84a5d830641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) \u001b[1mNLP\u001b[0m (2) \u001b[1mis\u001b[0m (3) \u001b[1mimportant\u001b[0m (4) \u001b[1mfor\u001b[0m (5) \u001b[1mscientific,\u001b[0m (6) \u001b[1meconomic,\u001b[0m (7) \u001b[1msocial,\u001b[0m (8) \u001b[1mand\u001b[0m (9) \u001b[1mcultural\u001b[0m (10) \u001b[1mreasons.\u001b[0m (11) \u001b[1mNLP\u001b[0m (12) \u001b[1mis\u001b[0m (13) \u001b[1mexperiencing\u001b[0m (14) \u001b[1mrapid\u001b[0m (15) \u001b[1mgrowth\u001b[0m (16) \u001b[1mas\u001b[0m (17) \u001b[1mits\u001b[0m (18) \u001b[1mtheories\u001b[0m (19) \u001b[1mand\u001b[0m (20) \u001b[1mmethods\u001b[0m (21) \u001b[1mare\u001b[0m (22) \u001b[1mdeployed\u001b[0m (23) \u001b[1min\u001b[0m (24) \u001b[1ma\u001b[0m (25) \u001b[1mvariety\u001b[0m (26) \u001b[1mof\u001b[0m (27) \u001b[1mnew\u001b[0m (28) \u001b[1mlanguage\u001b[0m (29) \u001b[1mtechnologies.\u001b[0m (30) \u001b[1mFor\u001b[0m (31) \u001b[1mthis\u001b[0m (32) \u001b[1mreason\u001b[0m (33) \u001b[1mit\u001b[0m (34) \u001b[1mis\u001b[0m (35) \u001b[1mimportant\u001b[0m (36) \u001b[1mfor\u001b[0m (37) \u001b[1ma\u001b[0m (38) \u001b[1mwide\u001b[0m (39) \u001b[1mrange\u001b[0m (40) \u001b[1mof\u001b[0m (41) \u001b[1mpeople\u001b[0m (42) \u001b[1mto\u001b[0m (43) \u001b[1mhave\u001b[0m (44) \u001b[1ma\u001b[0m (45) \u001b[1mworking\u001b[0m (46) \u001b[1mknowledge\u001b[0m (47) \u001b[1mof\u001b[0m (48) \u001b[1mNLP.\u001b[0m (49) \u001b[1mWithin\u001b[0m (50) \u001b[1mindustry,\u001b[0m (51) \u001b[1mthis\u001b[0m (52) \u001b[1mincludes\u001b[0m (53) \u001b[1mpeople\u001b[0m (54) \u001b[1min\u001b[0m (55) \u001b[1mhuman-computer\u001b[0m (56) \u001b[1minteraction,\u001b[0m (57) \u001b[1mbusiness\u001b[0m (58) \u001b[1minformation\u001b[0m (59) \u001b[1manalysis,\u001b[0m (60) \u001b[1mand\u001b[0m (61) \u001b[1mweb\u001b[0m (62) \u001b[1msoftware\u001b[0m (63) \u001b[1mdevelopment.\u001b[0m (64) \u001b[1mWithin\u001b[0m (65) \u001b[1macademia,\u001b[0m (66) \u001b[1mit\u001b[0m (67) \u001b[1mincludes\u001b[0m (68) \u001b[1mpeople\u001b[0m (69) \u001b[1min\u001b[0m (70) \u001b[1mareas\u001b[0m (71) \u001b[1mfrom\u001b[0m (72) \u001b[1mhumanities\u001b[0m (73) \u001b[1mcomputing\u001b[0m (74) \u001b[1mand\u001b[0m (75) \u001b[1mcorpus\u001b[0m (76) \u001b[1mlinguistics\u001b[0m (77) \u001b[1mthrough\u001b[0m (78) \u001b[1mto\u001b[0m (79) \u001b[1mcomputer\u001b[0m (80) \u001b[1mscience\u001b[0m (81) \u001b[1mand\u001b[0m (82) \u001b[1martificial\u001b[0m (83) \u001b[1mintelligence.\u001b[0m (84) \u001b[1m(To\u001b[0m (85) \u001b[1mmany\u001b[0m (86) \u001b[1mpeople\u001b[0m (87) \u001b[1min\u001b[0m (88) \u001b[1macademia,\u001b[0m (89) \u001b[1mNLP\u001b[0m (90) \u001b[1mis\u001b[0m (91) \u001b[1mknown\u001b[0m (92) \u001b[1mby\u001b[0m (93) \u001b[1mthe\u001b[0m (94) \u001b[1mname\u001b[0m (95) \u001b[1mof\u001b[0m (96) \u001b[1m\"Computational\u001b[0m (97) \u001b[1mLinguistics.\")\u001b[0m \n",
      "\n",
      "Total number of tokens in the paragraph:  97\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split() # Python split function\n",
    "\n",
    "# Print output tokens\n",
    "for i in range(len(tokens)):\n",
    "    n = i + 1\n",
    "    print('(%d) {}'.format(\"\\033[1m\" + tokens[i] + \"\\033[0m\") %n, end = \" \")\n",
    "    \n",
    "print('\\n\\nTotal number of tokens in the paragraph: ', len(tokens)) # Calculate total number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419231b-719d-48e7-b09b-a312855f31f7",
   "metadata": {},
   "source": [
    "#### b. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba58a06-1974-49c3-ab46-51dc74f208ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) \u001b[1mNLP\u001b[0m (2) \u001b[1mis\u001b[0m (3) \u001b[1mimportant\u001b[0m (4) \u001b[1mfor\u001b[0m (5) \u001b[1mscientific\u001b[0m (6) \u001b[1meconomic\u001b[0m (7) \u001b[1msocial\u001b[0m (8) \u001b[1mand\u001b[0m (9) \u001b[1mcultural\u001b[0m (10) \u001b[1mreasons\u001b[0m (11) \u001b[1mNLP\u001b[0m (12) \u001b[1mis\u001b[0m (13) \u001b[1mexperiencing\u001b[0m (14) \u001b[1mrapid\u001b[0m (15) \u001b[1mgrowth\u001b[0m (16) \u001b[1mas\u001b[0m (17) \u001b[1mits\u001b[0m (18) \u001b[1mtheories\u001b[0m (19) \u001b[1mand\u001b[0m (20) \u001b[1mmethods\u001b[0m (21) \u001b[1mare\u001b[0m (22) \u001b[1mdeployed\u001b[0m (23) \u001b[1min\u001b[0m (24) \u001b[1ma\u001b[0m (25) \u001b[1mvariety\u001b[0m (26) \u001b[1mof\u001b[0m (27) \u001b[1mnew\u001b[0m (28) \u001b[1mlanguage\u001b[0m (29) \u001b[1mtechnologies\u001b[0m (30) \u001b[1mFor\u001b[0m (31) \u001b[1mthis\u001b[0m (32) \u001b[1mreason\u001b[0m (33) \u001b[1mit\u001b[0m (34) \u001b[1mis\u001b[0m (35) \u001b[1mimportant\u001b[0m (36) \u001b[1mfor\u001b[0m (37) \u001b[1ma\u001b[0m (38) \u001b[1mwide\u001b[0m (39) \u001b[1mrange\u001b[0m (40) \u001b[1mof\u001b[0m (41) \u001b[1mpeople\u001b[0m (42) \u001b[1mto\u001b[0m (43) \u001b[1mhave\u001b[0m (44) \u001b[1ma\u001b[0m (45) \u001b[1mworking\u001b[0m (46) \u001b[1mknowledge\u001b[0m (47) \u001b[1mof\u001b[0m (48) \u001b[1mNLP\u001b[0m (49) \u001b[1mWithin\u001b[0m (50) \u001b[1mindustry\u001b[0m (51) \u001b[1mthis\u001b[0m (52) \u001b[1mincludes\u001b[0m (53) \u001b[1mpeople\u001b[0m (54) \u001b[1min\u001b[0m (55) \u001b[1mhuman-computer\u001b[0m (56) \u001b[1minteraction\u001b[0m (57) \u001b[1mbusiness\u001b[0m (58) \u001b[1minformation\u001b[0m (59) \u001b[1manalysis\u001b[0m (60) \u001b[1mand\u001b[0m (61) \u001b[1mweb\u001b[0m (62) \u001b[1msoftware\u001b[0m (63) \u001b[1mdevelopment\u001b[0m (64) \u001b[1mWithin\u001b[0m (65) \u001b[1macademia\u001b[0m (66) \u001b[1mit\u001b[0m (67) \u001b[1mincludes\u001b[0m (68) \u001b[1mpeople\u001b[0m (69) \u001b[1min\u001b[0m (70) \u001b[1mareas\u001b[0m (71) \u001b[1mfrom\u001b[0m (72) \u001b[1mhumanities\u001b[0m (73) \u001b[1mcomputing\u001b[0m (74) \u001b[1mand\u001b[0m (75) \u001b[1mcorpus\u001b[0m (76) \u001b[1mlinguistics\u001b[0m (77) \u001b[1mthrough\u001b[0m (78) \u001b[1mto\u001b[0m (79) \u001b[1mcomputer\u001b[0m (80) \u001b[1mscience\u001b[0m (81) \u001b[1mand\u001b[0m (82) \u001b[1martificial\u001b[0m (83) \u001b[1mintelligence\u001b[0m (84) \u001b[1mTo\u001b[0m (85) \u001b[1mmany\u001b[0m (86) \u001b[1mpeople\u001b[0m (87) \u001b[1min\u001b[0m (88) \u001b[1macademia\u001b[0m (89) \u001b[1mNLP\u001b[0m (90) \u001b[1mis\u001b[0m (91) \u001b[1mknown\u001b[0m (92) \u001b[1mby\u001b[0m (93) \u001b[1mthe\u001b[0m (94) \u001b[1mname\u001b[0m (95) \u001b[1mof\u001b[0m (96) \u001b[1mComputational\u001b[0m (97) \u001b[1mLinguistics\u001b[0m \n",
      "\n",
      "Total number of tokens in the paragraph:  97\n"
     ]
    }
   ],
   "source": [
    "pattern = r'[\\w-]+' # Regular Expression to split the tokens\n",
    "tokens = re.findall(pattern, text)\n",
    "\n",
    "# Print output tokens\n",
    "for i in range(len(tokens)):\n",
    "    n = i + 1\n",
    "    print('(%d) {}'.format(\"\\033[1m\" + tokens[i] + \"\\033[0m\") %n, end = \" \")\n",
    "    \n",
    "print('\\n\\nTotal number of tokens in the paragraph: ', len(tokens)) # Calculate total number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a873f30-7dc3-4b7e-be74-582570e446e9",
   "metadata": {},
   "source": [
    "#### c. NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48372049-cd53-438a-91bb-b3cbbfccef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) \u001b[1mNLP\u001b[0m (2) \u001b[1mis\u001b[0m (3) \u001b[1mimportant\u001b[0m (4) \u001b[1mfor\u001b[0m (5) \u001b[1mscientific\u001b[0m (6) \u001b[1m,\u001b[0m (7) \u001b[1meconomic\u001b[0m (8) \u001b[1m,\u001b[0m (9) \u001b[1msocial\u001b[0m (10) \u001b[1m,\u001b[0m (11) \u001b[1mand\u001b[0m (12) \u001b[1mcultural\u001b[0m (13) \u001b[1mreasons\u001b[0m (14) \u001b[1m.\u001b[0m (15) \u001b[1mNLP\u001b[0m (16) \u001b[1mis\u001b[0m (17) \u001b[1mexperiencing\u001b[0m (18) \u001b[1mrapid\u001b[0m (19) \u001b[1mgrowth\u001b[0m (20) \u001b[1mas\u001b[0m (21) \u001b[1mits\u001b[0m (22) \u001b[1mtheories\u001b[0m (23) \u001b[1mand\u001b[0m (24) \u001b[1mmethods\u001b[0m (25) \u001b[1mare\u001b[0m (26) \u001b[1mdeployed\u001b[0m (27) \u001b[1min\u001b[0m (28) \u001b[1ma\u001b[0m (29) \u001b[1mvariety\u001b[0m (30) \u001b[1mof\u001b[0m (31) \u001b[1mnew\u001b[0m (32) \u001b[1mlanguage\u001b[0m (33) \u001b[1mtechnologies\u001b[0m (34) \u001b[1m.\u001b[0m (35) \u001b[1mFor\u001b[0m (36) \u001b[1mthis\u001b[0m (37) \u001b[1mreason\u001b[0m (38) \u001b[1mit\u001b[0m (39) \u001b[1mis\u001b[0m (40) \u001b[1mimportant\u001b[0m (41) \u001b[1mfor\u001b[0m (42) \u001b[1ma\u001b[0m (43) \u001b[1mwide\u001b[0m (44) \u001b[1mrange\u001b[0m (45) \u001b[1mof\u001b[0m (46) \u001b[1mpeople\u001b[0m (47) \u001b[1mto\u001b[0m (48) \u001b[1mhave\u001b[0m (49) \u001b[1ma\u001b[0m (50) \u001b[1mworking\u001b[0m (51) \u001b[1mknowledge\u001b[0m (52) \u001b[1mof\u001b[0m (53) \u001b[1mNLP\u001b[0m (54) \u001b[1m.\u001b[0m (55) \u001b[1mWithin\u001b[0m (56) \u001b[1mindustry\u001b[0m (57) \u001b[1m,\u001b[0m (58) \u001b[1mthis\u001b[0m (59) \u001b[1mincludes\u001b[0m (60) \u001b[1mpeople\u001b[0m (61) \u001b[1min\u001b[0m (62) \u001b[1mhuman-computer\u001b[0m (63) \u001b[1minteraction\u001b[0m (64) \u001b[1m,\u001b[0m (65) \u001b[1mbusiness\u001b[0m (66) \u001b[1minformation\u001b[0m (67) \u001b[1manalysis\u001b[0m (68) \u001b[1m,\u001b[0m (69) \u001b[1mand\u001b[0m (70) \u001b[1mweb\u001b[0m (71) \u001b[1msoftware\u001b[0m (72) \u001b[1mdevelopment\u001b[0m (73) \u001b[1m.\u001b[0m (74) \u001b[1mWithin\u001b[0m (75) \u001b[1macademia\u001b[0m (76) \u001b[1m,\u001b[0m (77) \u001b[1mit\u001b[0m (78) \u001b[1mincludes\u001b[0m (79) \u001b[1mpeople\u001b[0m (80) \u001b[1min\u001b[0m (81) \u001b[1mareas\u001b[0m (82) \u001b[1mfrom\u001b[0m (83) \u001b[1mhumanities\u001b[0m (84) \u001b[1mcomputing\u001b[0m (85) \u001b[1mand\u001b[0m (86) \u001b[1mcorpus\u001b[0m (87) \u001b[1mlinguistics\u001b[0m (88) \u001b[1mthrough\u001b[0m (89) \u001b[1mto\u001b[0m (90) \u001b[1mcomputer\u001b[0m (91) \u001b[1mscience\u001b[0m (92) \u001b[1mand\u001b[0m (93) \u001b[1martificial\u001b[0m (94) \u001b[1mintelligence\u001b[0m (95) \u001b[1m.\u001b[0m (96) \u001b[1m(\u001b[0m (97) \u001b[1mTo\u001b[0m (98) \u001b[1mmany\u001b[0m (99) \u001b[1mpeople\u001b[0m (100) \u001b[1min\u001b[0m (101) \u001b[1macademia\u001b[0m (102) \u001b[1m,\u001b[0m (103) \u001b[1mNLP\u001b[0m (104) \u001b[1mis\u001b[0m (105) \u001b[1mknown\u001b[0m (106) \u001b[1mby\u001b[0m (107) \u001b[1mthe\u001b[0m (108) \u001b[1mname\u001b[0m (109) \u001b[1mof\u001b[0m (110) \u001b[1m``\u001b[0m (111) \u001b[1mComputational\u001b[0m (112) \u001b[1mLinguistics\u001b[0m (113) \u001b[1m.\u001b[0m (114) \u001b[1m''\u001b[0m (115) \u001b[1m)\u001b[0m \n",
      "\n",
      "Total number of tokens in the paragraph:  115\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(text) # word tokenization with NLTK \n",
    "\n",
    "# Print output tokens\n",
    "for i in range(len(tokens)):\n",
    "    n = i + 1\n",
    "    print('(%d) {}'.format(\"\\033[1m\" + tokens[i] + \"\\033[0m\") %n, end = \" \")\n",
    "    \n",
    "print('\\n\\nTotal number of tokens in the paragraph: ', len(tokens)) # Calculate total number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c51ee1-d2d0-4651-9b8c-abc32415bc5b",
   "metadata": {},
   "source": [
    "## Q2. FORM WORD STEMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a200b5-9816-4ca3-9031-6a78504230a4",
   "metadata": {},
   "source": [
    "### 2. Word Stemming (Regular Expression, Porter Stemmer & Lancaster Stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1941f-3484-47f2-a6db-057c6f38676e",
   "metadata": {},
   "source": [
    "#### a. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407576d5-6e73-447d-a79a-c5f6a1be8434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP i important for scientific , economic , social , and cultural reason . NLP i experienc rapid growth a it theor and method are deploy in a variety of new language technolog . For thi reason it i important for a wide range of people to have a work knowledge of NLP . Within industry , thi includ people in human-computer interaction , busines information analysi , and web software develop . Within academia , it includ people in area from humanit comput and corpu linguistic through to computer science and artificial intelligence . ( To many people in academia , NLP i known by the name of `` Computational Linguistic . '' )\n",
      "\n",
      "Characters count:  514\n",
      "Percentage of information retained:  92.11 %\n"
     ]
    }
   ],
   "source": [
    "# Regular Expression Stemmer\n",
    "def stemmer(word):\n",
    "    pattern = r'ing\\b|ly\\b|ed\\b|ious\\b|ies\\b|ive\\b|es\\b|s\\b|ment\\b'\n",
    "    re_stem = re.sub(pattern, '', word)\n",
    "    return re_stem\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(text) # Word tokenization\n",
    "\n",
    "re = [stemmer(stem) for stem in tokens] # Perform RE stemming by fitting the tokens into the RE stemmer function\n",
    "print(' '.join(re))\n",
    "print('\\nCharacters count: ', len(''.join(re))) # Character count\n",
    "\n",
    "info = (len(''.join(re))/len(''.join(tokens))) * 100\n",
    "print('Percentage of information retained: ', \"{:.2f}\".format(info), '%') # Compute percentage of information retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8faa53-2be6-4f2e-9d28-8709063d526c",
   "metadata": {},
   "source": [
    "#### b. Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d676aca0-2d52-4cab-8cf8-2c62045bc8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp is import for scientif , econom , social , and cultur reason . nlp is experienc rapid growth as it theori and method are deploy in a varieti of new languag technolog . for thi reason it is import for a wide rang of peopl to have a work knowledg of nlp . within industri , thi includ peopl in human-comput interact , busi inform analysi , and web softwar develop . within academia , it includ peopl in area from human comput and corpu linguist through to comput scienc and artifici intellig . ( to mani peopl in academia , nlp is known by the name of `` comput linguist . '' )\n",
      "\n",
      "Characters count:  465\n",
      "Percentage of information retained:  83.33 %\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(text) # Word tokenization \n",
    "\n",
    "ps = [stemmer.stem(stem) for stem in tokens] # Perform Porter stemming\n",
    "print(' '.join(ps))\n",
    "print('\\nCharacters count: ', len(''.join(ps))) # Character count\n",
    "\n",
    "info = (len(''.join(ps))/len(''.join(tokens))) * 100\n",
    "print('Percentage of information retained: ', \"{:.2f}\".format(info), '%') # Compute percentage of information retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e44c3-d759-4ffc-a1ae-0fddaa699b26",
   "metadata": {},
   "source": [
    "#### c. Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bea5a8-ecec-4ecc-87e8-6e0dd5b3b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp is import for sci , econom , soc , and cult reason . nlp is expery rapid grow as it the and method ar deploy in a vary of new langu technolog . for thi reason it is import for a wid rang of peopl to hav a work knowledg of nlp . within industry , thi includ peopl in human-computer interact , busy inform analys , and web softw develop . within academ , it includ peopl in area from hum comput and corp lingu through to comput sci and art intellig . ( to many peopl in academ , nlp is known by the nam of `` comput lingu . '' )\n",
      "\n",
      "Characters count:  416\n",
      "Percentage of information retained:  74.55 %\n"
     ]
    }
   ],
   "source": [
    "# Lancaster Stemmer\n",
    "stemmer = nltk.stem.LancasterStemmer()\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(text) # Word tokenization\n",
    "\n",
    "ls = [stemmer.stem(stem) for stem in tokens] # Perform Lancaster stemming\n",
    "print(' '.join(ls))\n",
    "print('\\nCharacters count: ', len(''.join(ls))) # Character count\n",
    "\n",
    "info = (len(''.join(ls))/len(''.join(tokens))) * 100\n",
    "print('Percentage of information retained: ', \"{:.2f}\".format(info), '%') # Compute percentage of information retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b78ed6-9659-47d6-b955-914830dd149c",
   "metadata": {},
   "source": [
    "## Q3. FILTER STOPWORDS AND PUNCTUATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d4280-d53c-4899-aff3-96ee1d1dec9e",
   "metadata": {},
   "source": [
    "### 1. Filter Stopwords and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32cebb9-d226-411d-b29c-094b2e173326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP important scientific economic social cultural reasons NLP experiencing rapid growth theories methods deployed variety new language technologies For reason important wide range people working knowledge NLP Within industry includes people human-computer interaction business information analysis web software development Within academia includes people areas humanities computing corpus linguistics computer science artificial intelligence To many people academia NLP known name Computational Linguistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# a. FILTER STOPWORDS\n",
    "nltk.download('stopwords') #download stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english') #Get English stopwords\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(text) # Word tokenization \n",
    "\n",
    "filtered_text = [w for w in tokens if not w in set(stopwords)] #Removing stopwords\n",
    "\n",
    "# b. FILTER PUNCTUATIONS \n",
    "punctuations = list(string.punctuation) #Get punctuations\n",
    "punctuations.append('``') #Adding additional punctuations \n",
    "punctuations.append(\"''\") #Adding additional punctuations\n",
    "filtered_text_no_punc = [w for w in filtered_text if not w in punctuations] #Removing punctuations\n",
    "print(' '.join(filtered_text_no_punc)) #Print final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642d67c-9732-42d7-b162-7799171d1929",
   "metadata": {},
   "source": [
    "### 2. Output of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d778de-1aa2-41bf-aa17-5faa4d8e4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of stopwords:  ['is', 'for', 'and', 'is', 'as', 'its', 'and', 'are', 'in', 'a', 'of', 'this', 'it', 'is', 'for', 'a', 'of', 'to', 'have', 'a', 'of', 'this', 'in', 'and', 'it', 'in', 'from', 'and', 'through', 'to', 'and', 'in', 'is', 'by', 'the', 'of']\n"
     ]
    }
   ],
   "source": [
    "stopwords_text = [sw for sw in tokens if sw in set(stopwords)]\n",
    "print('list of stopwords: ', stopwords_text) # Print list of stopwords exist in the text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f20f38-fa9e-4976-828e-c8eeb0dad674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique stopwords:  {'is', 'of', 'to', 'have', 'it', 'a', 'by', 'its', 'and', 'as', 'are', 'this', 'the', 'in', 'for', 'through', 'from'}\n"
     ]
    }
   ],
   "source": [
    "print('unique stopwords: ', set(stopwords_text)) # Print unique stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbd5fe-4069-4c5f-ae18-493e2a8306c9",
   "metadata": {},
   "source": [
    "## Q4. POS TAGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f822c3-5ab5-44d8-8e41-3560f1fd95bf",
   "metadata": {},
   "source": [
    "### Read txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98fd7ec4-5108-4fd2-8428-7befdf29ee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The little yellow dog barked at the cute cat and chased away.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read txt file\n",
    "file = open('D:\\Essien\\Degree Studies\\Degree Level 3\\Semester 1\\TXSA\\Individual Assignment\\Individual Assignment Data\\Data_2.txt')\n",
    "text2 = file.read()\n",
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d73b1-fad1-4e01-bf70-08d0b5eaadb0",
   "metadata": {},
   "source": [
    "### 1. Demonstrate POS Tagging (NLTK POS Tagger, textblob POS Tagger & Regular Expression POS Tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257918c2-eee5-4290-8a92-a94c2fdeaa16",
   "metadata": {},
   "source": [
    "#### a. NLTK POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ccce9e7-e94f-4387-878d-aaff494478ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "tokens = nltk.tokenize.word_tokenize(text2) #Word tokenization \n",
    "print(nltk.pos_tag(tokens)) #Print tokens with POS tag using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70248541-4c30-4bea-b83b-72ca6f4da758",
   "metadata": {},
   "source": [
    "#### b. Textblob POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cafb4beb-f38a-4238-9552-5c3fc1a9aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "tb = TextBlob(text2) #Create a Textblob object \n",
    "print(tb.tags) #Print tokens with POS tag using Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a315c-c057-4c25-a86f-af151b4b04a0",
   "metadata": {},
   "source": [
    "#### c. Regular Expression POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "799cde96-7abd-4a58-8f5e-72e5e931a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('little', 'NN'), ('yellow', 'NN'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'(A|a|An|an|The|the)$', 'DT'),  #Common determiners\n",
    "     (r'(with|at|by|to|in|for|from|of|on)', 'IN'), #common prepositions \n",
    "     (r'(and|or|so|because|therefore|yet)', 'CC'), #common conjunctions \n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN'),                    # nouns (default)\n",
    "     (r'^\\d+$', 'CD'),\n",
    "     (r'.*ing$', 'VBG'),               # gerunds, i.e. wondering\n",
    "     (r'.*ment$', 'NN'),               # i.e. wonderment\n",
    "     (r'.*ful$', 'JJ')                # i.e. wonderful\n",
    " ]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "tagger = nltk.tag.sequential.RegexpTagger(patterns) #Fit RE patterns into tagger \n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(text2) #Word tokenization \n",
    "print(tagger.tag(tokens)) #Print tokens with POS tag using RE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6755be2-2993-498e-9d92-3af786150e20",
   "metadata": {},
   "source": [
    "### 4. Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93bdc628-6d51-49e1-9ae3-b351aab0dd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'little', 'yellow', 'dog', 'barked', 'at', 'the', 'cute', 'cat', 'and', 'chased', 'away']\n",
      "(S\n",
      "  (NP (DT The) (Nom (Adj little) (Adj yellow) (NN dog)))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V barked))\n",
      "      (PP (IN at) (NP (DT the) (Nom (Adj cute) (NN cat)))))\n",
      "    (CC and)\n",
      "    (VP (VP (V chased)) (RB away))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP \n",
    "PP -> IN NP \n",
    "NP -> DT NN | NP PP | DT Nom\n",
    "VP -> V NP | VP PP | VP RB | VP CC VP | V \n",
    "Nom -> Adj NN | Adj Adj NN\n",
    "\n",
    "DT -> 'The' | 'the'\n",
    "Adj -> 'little' | 'yellow' | 'cute'\n",
    "NN -> 'dog' | 'cat'\n",
    "V -> 'barked' | 'chased'\n",
    "IN -> 'at'\n",
    "CC -> 'and'\n",
    "RB -> 'away'\n",
    "\"\"\")\n",
    "\n",
    "text1 = nltk.tokenize.word_tokenize(text2) #word tokenization \n",
    "punctuations = list(string.punctuation) #Get punctuations\n",
    "filtered_text = [w for w in text1 if not w in punctuations] #Removing punctuations\n",
    "\n",
    "print(filtered_text)\n",
    "parser = nltk.ChartParser(grammar) #Fit grammar rules\n",
    "for tree in parser.parse(filtered_text):\n",
    "    tree.draw()\n",
    "print(tree) #Print parse tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
